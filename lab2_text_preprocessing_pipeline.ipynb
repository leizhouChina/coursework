{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOYEAxQ5WPGd"
      },
      "source": [
        "# Lab2: Text Preprocessing Pipeline\n",
        "\n",
        "**Duration:** 1 hour\n",
        "\n",
        "**Objectives:**\n",
        "- Understand and implement tokenization techniques\n",
        "- Apply stemming and lemmatization for text normalization\n",
        "- Remove stop words and special characters\n",
        "- Build a complete text preprocessing pipeline\n",
        "\n",
        "---\n",
        "\n",
        "## Instructions\n",
        "\n",
        "1. Complete all the exercises marked with `# TODO`\n",
        "2. Run each cell to verify your answers\n",
        "3. Save your completed notebook\n",
        "4. **Push your work to a Git repository and send the link to: yoroba93@gmail.com**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYnIY92EWPGf"
      },
      "source": [
        "## Setup: Install and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6twLXK1pWPGf",
        "outputId": "26fb6370-1ead-4d3a-eb57-bcde6133a84a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries (uncomment if needed)\n",
        "# !pip install nltk spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8TVmlo6WPGg",
        "outputId": "b3283d5e-6f6f-4e8b-b97d-dd98cde98b2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import re\n",
        "import string\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Download and load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlJY5PbYWPGg"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 1: Tokenization\n",
        "\n",
        "Tokenization is the process of breaking text into smaller units (tokens), typically words or sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIfhLtWEWPGh"
      },
      "source": [
        "## 1.1 Basic Tokenization with Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyi6BgIwWPGh",
        "outputId": "cf1bf2e3-dc75-4b2c-9809-bc510e4335f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Basic split(): ['Natural', 'Language', 'Processing', 'is', 'fascinating!']\n",
            "Last token: fascinating!\n"
          ]
        }
      ],
      "source": [
        "# Simple tokenization using split()\n",
        "text = \"Natural Language Processing is fascinating!\"\n",
        "\n",
        "# Basic split on whitespace\n",
        "tokens_basic = text.split()\n",
        "print(\"Basic split():\", tokens_basic)\n",
        "\n",
        "# Problem: punctuation is attached to words!\n",
        "print(\"Last token:\", tokens_basic[-1])  # 'fascinating!' includes the !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M2E7XjQWPGh"
      },
      "source": [
        "## 1.2 Tokenization with NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0Kuc8OOWPGh",
        "outputId": "7a95f77f-f772-4094-933f-ba83362306fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokens: ['Hello', '!', 'How', 'are', 'you', 'doing', 'today', '?', 'I', \"'m\", 'learning', 'NLP', '.', 'It', \"'s\", 'really', 'interesting', '.']\n",
            "\n",
            "Sentence tokens:\n",
            "  1. Hello!\n",
            "  2. How are you doing today?\n",
            "  3. I'm learning NLP.\n",
            "  4. It's really interesting.\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"Hello! How are you doing today? I'm learning NLP. It's really interesting.\"\n",
        "\n",
        "# Word tokenization\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"Word tokens:\", word_tokens)\n",
        "\n",
        "# Sentence tokenization\n",
        "sent_tokens = sent_tokenize(text)\n",
        "print(\"\\nSentence tokens:\")\n",
        "for i, sent in enumerate(sent_tokens):\n",
        "    print(f\"  {i+1}. {sent}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlxQYV2jWPGi",
        "outputId": "85026f58-9311-4736-be2a-fbbad144df7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Basic split: ['I', \"can't\", 'believe', \"it's\", 'not', 'butter!', \"Don't\", 'you', 'think', 'so?']\n",
            "NLTK tokenize: ['I', 'ca', \"n't\", 'believe', 'it', \"'s\", 'not', 'butter', '!', 'Do', \"n't\", 'you', 'think', 'so', '?']\n"
          ]
        }
      ],
      "source": [
        "# NLTK handles contractions and punctuation better\n",
        "text = \"I can't believe it's not butter! Don't you think so?\"\n",
        "\n",
        "print(\"Basic split:\", text.split())\n",
        "print(\"NLTK tokenize:\", word_tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3PrcbBeWPGi",
        "outputId": "dd215791-b02b-4e4a-fb3a-a132d7a9ee61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Dr.', 'Smith', \"'s\", 'patients', 'ca', \"n't\", 'understand', 'why', 'they', \"'re\", 'feeling', 'unwell', '.', 'Is', 'it', 'the', 'flu', '?']\n",
            "Number of tokens: 18\n"
          ]
        }
      ],
      "source": [
        "# TODO: Exercise 1.1\n",
        "# Tokenize the following text into words using NLTK\n",
        "# Count how many tokens are produced\n",
        "\n",
        "text = \"Dr. Smith's patients can't understand why they're feeling unwell. Is it the flu?\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "num_tokens = len(tokens)\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Number of tokens:\", num_tokens)\n",
        "\n",
        "assert num_tokens == 18, f\"Expected 18 tokens, got {num_tokens}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0RHNV2SWPGi"
      },
      "source": [
        "## 1.3 Tokenization with spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2pcUYryWPGi",
        "outputId": "c3370574-9d30-477e-9e20-806c4ed40f8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion', '.']\n",
            "\n",
            "Detailed token info:\n",
            "  Apple        | POS: PROPN  | Is Stop: False\n",
            "  is           | POS: AUX    | Is Stop: True\n",
            "  looking      | POS: VERB   | Is Stop: False\n",
            "  at           | POS: ADP    | Is Stop: True\n",
            "  buying       | POS: VERB   | Is Stop: False\n",
            "  U.K.         | POS: PROPN  | Is Stop: False\n",
            "  startup      | POS: VERB   | Is Stop: False\n",
            "  for          | POS: ADP    | Is Stop: True\n",
            "  $            | POS: SYM    | Is Stop: False\n",
            "  1            | POS: NUM    | Is Stop: False\n",
            "  billion      | POS: NUM    | Is Stop: False\n",
            "  .            | POS: PUNCT  | Is Stop: False\n"
          ]
        }
      ],
      "source": [
        "# spaCy tokenization\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "# Get tokens\n",
        "tokens = [token.text for token in doc]\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# spaCy provides additional information\n",
        "print(\"\\nDetailed token info:\")\n",
        "for token in doc:\n",
        "    print(f\"  {token.text:12} | POS: {token.pos_:6} | Is Stop: {token.is_stop}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq-tVd_RWPGj",
        "outputId": "68578077-2fd3-43ba-b25e-46f0f468ca13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'Is', \"n't\", 'it', 'amazing']\n"
          ]
        }
      ],
      "source": [
        "# TODO: Exercise 1.2\n",
        "# Use spaCy to tokenize the text and extract only:\n",
        "# 1. Tokens that are NOT punctuation\n",
        "# 2. Tokens that are NOT spaces\n",
        "# Hint: use token.is_punct and token.is_space\n",
        "\n",
        "text = \"The quick, brown fox jumps over the lazy dog! Isn't it amazing?\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "clean_tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
        "\n",
        "print(\"Clean tokens:\", clean_tokens)\n",
        "assert len(clean_tokens) == 13, f\"Expected 13 tokens, got {len(clean_tokens)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXClTbjAWPGj"
      },
      "source": [
        "## 1.4 Different Tokenization Strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmnWBjo1WPGj",
        "outputId": "947041f8-2f80-4f75-fb91-4a0db3b693ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regexp tokens: ['Hello', 'How', 's', 'it', 'going', 'NLP', 'user123']\n",
            "Tweet tokens: ['Hello', '!', \"How's\", 'it', 'going', '?', '#NLP', '@user123']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import RegexpTokenizer, TweetTokenizer\n",
        "\n",
        "# RegexpTokenizer - tokenize using a custom pattern\n",
        "# \\w+ matches word characters only (removes punctuation)\n",
        "regexp_tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "text = \"Hello! How's it going? #NLP @user123\"\n",
        "print(\"Regexp tokens:\", regexp_tokenizer.tokenize(text))\n",
        "\n",
        "# TweetTokenizer - designed for social media text\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "print(\"Tweet tokens:\", tweet_tokenizer.tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U41JvsAFWPGk",
        "outputId": "a9965faa-4d60-4281-829b-d28b8c916365"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alphabetic tokens: ['I', 'have', 'cats', 'and', 'dogs', 'Their', 'names', 'are', 'Max', 'and', 'Bella']\n"
          ]
        }
      ],
      "source": [
        "# TODO: Exercise 1.3\n",
        "# Create a RegexpTokenizer that extracts only alphabetic words (no numbers, no punctuation)\n",
        "# Pattern hint: [a-zA-Z]+ matches one or more letters\n",
        "\n",
        "text = \"I have 3 cats and 2 dogs! Their names are Max123 and Bella.\"\n",
        "\n",
        "alpha_tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
        "alpha_tokens = alpha_tokenizer.tokenize(text)\n",
        "\n",
        "print(\"Alphabetic tokens:\", alpha_tokens)\n",
        "assert alpha_tokens == ['I', 'have', 'cats', 'and', 'dogs', 'Their', 'names', 'are', 'Max', 'and', 'Bella'], \"Check your pattern!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFye2_XDWPGk"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 2: Stemming and Lemmatization\n",
        "\n",
        "Both techniques reduce words to their base form, but they work differently:\n",
        "- **Stemming**: Chops off word endings using rules (faster, cruder)\n",
        "- **Lemmatization**: Uses vocabulary and morphological analysis (slower, accurate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YId1YsuZWPGk"
      },
      "source": [
        "## 2.1 Stemming with NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq24XfkzWPGk",
        "outputId": "4d3a53f7-2f00-456e-86a2-828d201c0517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer:\n",
            "  running      -> run\n",
            "  runs         -> run\n",
            "  ran          -> ran\n",
            "  runner       -> runner\n",
            "  easily       -> easili\n",
            "  fairly       -> fairli\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "\n",
        "# Porter Stemmer (most common)\n",
        "porter = PorterStemmer()\n",
        "\n",
        "words = [\"running\", \"runs\", \"ran\", \"runner\", \"easily\", \"fairly\"]\n",
        "\n",
        "print(\"Porter Stemmer:\")\n",
        "for word in words:\n",
        "    print(f\"  {word:12} -> {porter.stem(word)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFOGON-KWPGk",
        "outputId": "fb7c39a6-90d2-45e1-8b7d-47366fabee0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Snowball Stemmer:\n",
            "  running      -> run\n",
            "  runs         -> run\n",
            "  ran          -> ran\n",
            "  runner       -> runner\n",
            "  easily       -> easili\n",
            "  fairly       -> fair\n",
            "\n",
            "Available languages: ('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
          ]
        }
      ],
      "source": [
        "# Snowball Stemmer (supports multiple languages)\n",
        "snowball = SnowballStemmer('english')\n",
        "\n",
        "print(\"Snowball Stemmer:\")\n",
        "for word in words:\n",
        "    print(f\"  {word:12} -> {snowball.stem(word)}\")\n",
        "\n",
        "# Available languages\n",
        "print(\"\\nAvailable languages:\", SnowballStemmer.languages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkQ41LpEWPGl",
        "outputId": "f042aed0-152b-4742-ba25-14c7c43d04ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming can produce non-words:\n",
            "  studies      -> studi\n",
            "  studying     -> studi\n",
            "  university   -> univers\n",
            "  universe     -> univers\n",
            "  beautiful    -> beauti\n",
            "  beauty       -> beauti\n"
          ]
        }
      ],
      "source": [
        "# Stemming limitations - can produce non-words\n",
        "problem_words = [\"studies\", \"studying\", \"university\", \"universe\", \"beautiful\", \"beauty\"]\n",
        "\n",
        "print(\"Stemming can produce non-words:\")\n",
        "for word in problem_words:\n",
        "    print(f\"  {word:12} -> {porter.stem(word)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2JD_axtWPGl",
        "outputId": "63b353c5-69a0-4d98-f688-a22041c7dad2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: ['The', 'cats', 'are', 'running', 'and', 'jumping', 'over', 'the', 'sleeping', 'dogs']\n",
            "Stemmed: ['the', 'cat', 'are', 'run', 'and', 'jump', 'over', 'the', 'sleep', 'dog']\n"
          ]
        }
      ],
      "source": [
        "# TODO: Exercise 2.1\n",
        "# Apply Porter stemming to all words in the sentence\n",
        "# Return the stemmed tokens as a list\n",
        "\n",
        "sentence = \"The cats are running and jumping over the sleeping dogs\"\n",
        "\n",
        "# Step 1: Tokenize (use word_tokenize)\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "# Step 2: Apply stemming to each token\n",
        "stemmed_tokens = [porter.stem(word) for word in tokens]\n",
        "\n",
        "print(\"Original:\", tokens)\n",
        "print(\"Stemmed:\", stemmed_tokens)\n",
        "\n",
        "assert stemmed_tokens == ['the', 'cat', 'are', 'run', 'and', 'jump', 'over', 'the', 'sleep', 'dog'], \"Check your stemming!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Yf7y7AEWPGl"
      },
      "source": [
        "## 2.2 Lemmatization with NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXcPdVrTWPGl",
        "outputId": "00050951-eada-49bb-f1d6-472ee29c164b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization (default - assumes nouns):\n",
            "  running      -> running\n",
            "  runs         -> run\n",
            "  ran          -> ran\n",
            "  better       -> better\n",
            "  studies      -> study\n",
            "  feet         -> foot\n",
            "  geese        -> goose\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [\"running\", \"runs\", \"ran\", \"better\", \"studies\", \"feet\", \"geese\"]\n",
        "\n",
        "print(\"Lemmatization (default - assumes nouns):\")\n",
        "for word in words:\n",
        "    print(f\"  {word:12} -> {lemmatizer.lemmatize(word)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6gKCT9dWPGl",
        "outputId": "fb8269f2-48e6-4693-ae66-1aba2df061dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization with POS tags:\n",
            "  running (verb):     run\n",
            "  running (noun):     running\n",
            "  better (adjective): good\n",
            "  studies (verb):     study\n",
            "  studies (noun):     study\n"
          ]
        }
      ],
      "source": [
        "# Lemmatization works better with POS tags\n",
        "# pos: 'n' = noun, 'v' = verb, 'a' = adjective, 'r' = adverb\n",
        "\n",
        "print(\"Lemmatization with POS tags:\")\n",
        "print(f\"  running (verb):     {lemmatizer.lemmatize('running', pos='v')}\")\n",
        "print(f\"  running (noun):     {lemmatizer.lemmatize('running', pos='n')}\")\n",
        "print(f\"  better (adjective): {lemmatizer.lemmatize('better', pos='a')}\")\n",
        "print(f\"  studies (verb):     {lemmatizer.lemmatize('studies', pos='v')}\")\n",
        "print(f\"  studies (noun):     {lemmatizer.lemmatize('studies', pos='n')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ij5cxtCpWPGl",
        "outputId": "623afe3d-10e0-44ec-a2f4-fcce586b8dae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization results:\n",
            "  flying       (v) -> fly\n",
            "  happily      (r) -> happily\n",
            "  worse        (a) -> bad\n",
            "  mice         (n) -> mouse\n",
            "  are          (v) -> be\n"
          ]
        }
      ],
      "source": [
        "# TODO: Exercise 2.2\n",
        "# Lemmatize the following words using the correct POS tag\n",
        "# Fill in the POS tag for each word\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Format: (word, pos_tag)\n",
        "words_with_pos = [\n",
        "    (\"flying\", \"v\"),      # verb -> fly\n",
        "    (\"happily\", \"r\"),   # adverb -> happily (adverbs don't change much)\n",
        "    (\"worse\", \"a\"),     # adjective -> bad\n",
        "    (\"mice\", \"n\"),      # noun -> mouse\n",
        "    (\"are\", \"v\"),       # verb -> be\n",
        "]\n",
        "\n",
        "print(\"Lemmatization results:\")\n",
        "for word, pos in words_with_pos:\n",
        "    lemma = lemmatizer.lemmatize(word, pos=pos)\n",
        "    print(f\"  {word:12} ({pos}) -> {lemma}\")\n",
        "\n",
        "# Verify your answers\n",
        "expected = ['fly', 'happily', 'bad', 'mouse', 'be']\n",
        "results = [lemmatizer.lemmatize(w, pos=p) for w, p in words_with_pos]\n",
        "assert results == expected, f\"Expected {expected}, got {results}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5ywlmN0WPGm"
      },
      "source": [
        "## 2.3 Lemmatization with spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw6H0F1cWPGm",
        "outputId": "ebb510ab-d022-4656-fdd8-5082a66e1987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy lemmatization (automatic POS detection):\n",
            "  The          (DET  ) -> the\n",
            "  children     (NOUN ) -> child\n",
            "  are          (AUX  ) -> be\n",
            "  playing      (VERB ) -> play\n",
            "  toys         (NOUN ) -> toy\n",
            "  They         (PRON ) -> they\n",
            "  were         (AUX  ) -> be\n",
            "  running      (VERB ) -> run\n",
            "  jumping      (VERB ) -> jump\n"
          ]
        }
      ],
      "source": [
        "# spaCy automatically determines POS and lemmatizes correctly\n",
        "text = \"The children are playing with their toys. They were running and jumping happily.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"spaCy lemmatization (automatic POS detection):\")\n",
        "for token in doc:\n",
        "    if token.text != token.lemma_:  # Only show words that change\n",
        "        print(f\"  {token.text:12} ({token.pos_:5}) -> {token.lemma_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUsMO86dWPGm",
        "outputId": "dd57eb58-96ff-4f8c-d760-8df2f28f52ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmas: ['the', 'dog', 'be', 'bark', 'loudly', 'at', 'the', 'cat', 'who', 'be', 'climb', 'the', 'tree']\n"
          ]
        }
      ],
      "source": [
        "# TODO: Exercise 2.3\n",
        "# Use spaCy to extract the lemmas of all non-punctuation tokens\n",
        "# Return as a list of lowercase lemmas\n",
        "\n",
        "text = \"The dogs were barking loudly at the cats who were climbing the trees.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "lemmas = [token.lemma_.lower() for token in doc if not token.is_punct]\n",
        "\n",
        "print(\"Lemmas:\", lemmas)\n",
        "assert lemmas == ['the', 'dog', 'be', 'bark', 'loudly', 'at', 'the', 'cat', 'who', 'be', 'climb', 'the', 'tree'], \"Check your lemmatization!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODNe8gpSWPGm"
      },
      "source": [
        "## 2.4 Stemming vs Lemmatization Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbhQ4PucWPGn",
        "outputId": "b03fd552-fc38-4a99-db66-64166a2b6375"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word         Stemmed      Lemmatized  \n",
            "------------------------------------\n",
            "studies      studi        study       \n",
            "studying     studi        studying    \n",
            "better       better       better      \n",
            "feet         feet         foot        \n",
            "ran          ran          ran         \n",
            "easily       easili       easily      \n",
            "fairly       fairli       fairly      \n",
            "wolves       wolv         wolf        \n"
          ]
        }
      ],
      "source": [
        "# Compare the two approaches\n",
        "words = [\"studies\", \"studying\", \"better\", \"feet\", \"ran\", \"easily\", \"fairly\", \"wolves\"]\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(f\"{'Word':<12} {'Stemmed':<12} {'Lemmatized':<12}\")\n",
        "print(\"-\" * 36)\n",
        "for word in words:\n",
        "    stemmed = porter.stem(word)\n",
        "    # For comparison, we'll use noun as default\n",
        "    lemmatized = lemmatizer.lemmatize(word)\n",
        "    print(f\"{word:<12} {stemmed:<12} {lemmatized:<12}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-Um4YJcWPGn"
      },
      "source": [
        "**Key Differences:**\n",
        "- Stemming is faster but may produce non-words (\"studi\", \"easili\")\n",
        "- Lemmatization produces valid words but is slower\n",
        "- Lemmatization requires POS information for best results\n",
        "- Use stemming for speed (search engines), lemmatization for accuracy (chatbots, NLU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ5twauIWPGn"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 3: Stop Words and Special Characters\n",
        "\n",
        "Stop words are common words that usually don't carry much meaning (the, is, at, which, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BtaVbD2WPGn"
      },
      "source": [
        "## 3.1 Stop Words with NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1-k5Wq5WPGn",
        "outputId": "21c61830-6f29-42f0-99f6-b7774cd340a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of NLTK stop words: 198\n",
            "\n",
            "Sample stop words: [\"i'm\", 'more', 've', 'below', 'if', 'yourself', 'own', 'on', 'further', 'same', 'under', 'for', 'those', \"needn't\", 'they', 'whom', 'being', 'itself', 'through', 'this']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Get English stop words\n",
        "stop_words_nltk = set(stopwords.words('english'))\n",
        "\n",
        "print(f\"Number of NLTK stop words: {len(stop_words_nltk)}\")\n",
        "print(f\"\\nSample stop words: {list(stop_words_nltk)[:20]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gipe6QROWPGo",
        "outputId": "b837de61-1656-4007-bdd8-52f7fdff8a8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tokens: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'in', 'the', 'park']\n",
            "Without stop words: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', 'park']\n"
          ]
        }
      ],
      "source": [
        "# Remove stop words from text\n",
        "text = \"The quick brown fox jumps over the lazy dog in the park\"\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "# Filter out stop words\n",
        "filtered_tokens = [token for token in tokens if token not in stop_words_nltk]\n",
        "\n",
        "print(\"Original tokens:\", tokens)\n",
        "print(\"Without stop words:\", filtered_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu21uF8rWPGo",
        "outputId": "7b8a6658-dcc5-479e-e4af-1dd94683b443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered tokens: ['sample', 'sentence', 'showing', 'removal', 'stop', 'words', 'text']\n"
          ]
        }
      ],
      "source": [
        "# TODO: Exercise 3.1\n",
        "# Remove stop words from the following text and return the remaining tokens\n",
        "# Make sure to lowercase the text first!\n",
        "\n",
        "text = \"This is a sample sentence showing the removal of stop words from the text\"\n",
        "\n",
        "# Step 1: Lowercase and tokenize\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "# Step 2: Remove stop words\n",
        "filtered = [word for word in tokens if word not in stop_words_nltk]\n",
        "\n",
        "print(\"Filtered tokens:\", filtered)\n",
        "assert filtered == ['sample', 'sentence', 'showing', 'removal', 'stop', 'words', 'text'], \"Check your filtering!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SubUa0TYWPGo"
      },
      "source": [
        "## 3.2 Stop Words with spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R07HWapgWPGu",
        "outputId": "d27e5156-37ec-4586-c667-3a9c820347bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token analysis:\n",
            "  This            is_stop: True\n",
            "  is              is_stop: True\n",
            "  a               is_stop: True\n",
            "  sample          is_stop: False\n",
            "  sentence        is_stop: False\n",
            "  for             is_stop: True\n",
            "  demonstrating   is_stop: False\n",
            "  stop            is_stop: False\n",
            "  word            is_stop: False\n",
            "  removal         is_stop: False\n",
            "  .               is_stop: False\n"
          ]
        }
      ],
      "source": [
        "# spaCy has built-in stop word detection\n",
        "text = \"This is a sample sentence for demonstrating stop word removal.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"Token analysis:\")\n",
        "for token in doc:\n",
        "    print(f\"  {token.text:<15} is_stop: {token.is_stop}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Yr0FCWGWPGv",
        "outputId": "ebaac592-b525-42bd-f8e2-25fbb59e8968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content words: ['sample', 'sentence', 'demonstrating', 'stop', 'word', 'removal']\n"
          ]
        }
      ],
      "source": [
        "# Filter using spaCy's is_stop attribute\n",
        "content_words = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "print(\"Content words:\", content_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjmV_CcUWPGv",
        "outputId": "b1900d83-e93e-4daa-ca08-94804fe5723e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of spaCy stop words: 326\n"
          ]
        }
      ],
      "source": [
        "# Customize stop words in spaCy\n",
        "# Add custom stop words\n",
        "nlp.vocab[\"sample\"].is_stop = True\n",
        "\n",
        "# Remove words from stop list\n",
        "nlp.vocab[\"not\"].is_stop = False  # 'not' carries meaning!\n",
        "\n",
        "# Check the spaCy stop words list\n",
        "print(f\"Number of spaCy stop words: {len(nlp.Defaults.stop_words)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk_MF0wgWPGv"
      },
      "source": [
        "## 3.3 Removing Special Characters and Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-7StE3sWPGv",
        "outputId": "c5e7b538-eb30-49f6-aa50-6ab85c0be5ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation characters: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "\n",
            "Using translate(): Hello World Hows it going NLP user\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "# Python's string.punctuation\n",
        "print(\"Punctuation characters:\", string.punctuation)\n",
        "\n",
        "# Method 1: Using str.translate()\n",
        "text = \"Hello, World! How's it going? #NLP @user\"\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "clean_text = text.translate(translator)\n",
        "print(\"\\nUsing translate():\", clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozLS8mCiWPGv",
        "outputId": "26aaf54c-aead-43b4-b4ad-69a4ad35cd18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regex (letters only): Hello World Hows it going NLP user \n",
            "Regex (alphanumeric): Hello World Hows it going NLP user 123\n"
          ]
        }
      ],
      "source": [
        "# Method 2: Using regex\n",
        "import re\n",
        "\n",
        "text = \"Hello, World! How's it going? #NLP @user 123\"\n",
        "\n",
        "# Remove all non-alphanumeric characters (keep spaces)\n",
        "clean_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "print(\"Regex (letters only):\", clean_text)\n",
        "\n",
        "# Remove punctuation but keep numbers\n",
        "clean_text2 = re.sub(r'[^\\w\\s]', '', text)\n",
        "print(\"Regex (alphanumeric):\", clean_text2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcdA2KeLWPGv",
        "outputId": "40bbf2dd-976c-48da-fad5-c604d70f30ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean tokens: ['Hello', 'This', 'is', '@user', \"'s\", 'tweet', 'about', 'NLP', 'Check']\n"
          ]
        }
      ],
      "source": [
        "# Method 3: Using spaCy token attributes\n",
        "text = \"Hello! This is @user's tweet about #NLP. Check https://example.com!\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "# Filter tokens\n",
        "clean_tokens = [\n",
        "    token.text for token in doc\n",
        "    if not token.is_punct\n",
        "    and not token.is_space\n",
        "    and not token.like_url\n",
        "]\n",
        "\n",
        "print(\"Clean tokens:\", clean_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX5Vq93_WPGw",
        "outputId": "572b18da-c00c-429a-8b79-1325a5001d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean text: 'check out new model its amazing'\n"
          ]
        }
      ],
      "source": [
        "# TODO: Exercise 3.2\n",
        "# Clean the following text by:\n",
        "# 1. Removing URLs\n",
        "# 2. Removing mentions (@user)\n",
        "# 3. Removing hashtags (#topic)\n",
        "# 4. Removing punctuation\n",
        "# 5. Converting to lowercase\n",
        "# Use regex for this exercise\n",
        "\n",
        "text = \"Check out @OpenAI's new model! https://openai.com #AI #MachineLearning It's amazing!!!\"\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# Hint: Apply multiple re.sub() operations\n",
        "clean = text\n",
        "clean = re.sub(r'https?://\\S+', '', clean)\n",
        "clean = re.sub(r\"@\\w+('s)?\", '', clean)\n",
        "clean = re.sub(r'#\\w+', '', clean)\n",
        "clean = re.sub(r'[^\\w\\s]', '', clean)\n",
        "clean = clean.lower()\n",
        "clean = re.sub(r'\\s+', ' ', clean)\n",
        "clean = clean.strip()\n",
        "\n",
        "print(f\"Clean text: '{clean}'\")\n",
        "assert clean == \"check out new model its amazing\", f\"Got: '{clean}'\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA3Jxj37WPGw"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 4: Complete Preprocessing Pipeline\n",
        "\n",
        "Now let's combine everything into a complete text preprocessing pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gFqNkB2WPGw"
      },
      "source": [
        "## 4.1 Example Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwMuoinIWPGw",
        "outputId": "c91f8d0e-4231-4dd9-c2b4-13e3d04187a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed tokens: ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog', 'check', 'information', 'amazing', 'nlp', 'python', 'user']\n"
          ]
        }
      ],
      "source": [
        "def preprocess_pipeline_example(text):\n",
        "    \"\"\"\n",
        "    Example preprocessing pipeline.\n",
        "\n",
        "    Steps:\n",
        "    1. Lowercase\n",
        "    2. Remove URLs\n",
        "    3. Remove special characters\n",
        "    4. Tokenize\n",
        "    5. Remove stop words\n",
        "    6. Lemmatize\n",
        "    \"\"\"\n",
        "    # Step 1: Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Step 2: Remove URLs\n",
        "    text = re.sub(r'https?://\\S+', '', text)\n",
        "\n",
        "    # Step 3: Remove special characters (keep only letters and spaces)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    # Step 4: Tokenize with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Step 5 & 6: Remove stop words and lemmatize\n",
        "    tokens = [\n",
        "        token.lemma_\n",
        "        for token in doc\n",
        "        if not token.is_stop\n",
        "        and not token.is_punct\n",
        "        and not token.is_space\n",
        "        and len(token.text) > 1  # Remove single characters\n",
        "    ]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Test the pipeline\n",
        "sample_text = \"\"\"\n",
        "The quick brown foxes are jumping over the lazy dogs!\n",
        "Check out https://example.com for more information.\n",
        "This is SO amazing!!! #NLP #Python @user123\n",
        "\"\"\"\n",
        "\n",
        "result = preprocess_pipeline_example(sample_text)\n",
        "print(\"Preprocessed tokens:\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjF0HpnGWPGw"
      },
      "source": [
        "## 4.2 Final Challenge: Build Your Own Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "sao4l1VMWPGw"
      },
      "outputs": [],
      "source": [
        "# TODO: Exercise 4.1 (FINAL CHALLENGE)\n",
        "# Create a complete preprocessing pipeline function that:\n",
        "# 1. Converts text to lowercase\n",
        "# 2. Removes URLs (http/https)\n",
        "# 3. Removes email addresses\n",
        "# 4. Removes mentions (@user) and hashtags (#topic)\n",
        "# 5. Removes numbers\n",
        "# 6. Removes punctuation and special characters\n",
        "# 7. Tokenizes the text\n",
        "# 8. Removes stop words\n",
        "# 9. Applies lemmatization\n",
        "# 10. Removes tokens with less than 2 characters\n",
        "#\n",
        "# The function should return a list of cleaned tokens\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Complete text preprocessing pipeline.\n",
        "\n",
        "    Args:\n",
        "        text (str): Raw input text\n",
        "\n",
        "    Returns:\n",
        "        list: List of preprocessed tokens\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # Step 1: Lowercase\n",
        "    text = text.lower()\n",
        "    # Step 2: Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\s+', '', text)\n",
        "    # Step 3: Remove emails\n",
        "    text  = re.sub(r'\\S+@\\S+', '', text)\n",
        "    # Step 4: Remove mentions and hashtags\n",
        "    text  = re.sub(r'[@#]\\w+', '', text)\n",
        "    # Step 5: Remove numbers\n",
        "    text  = re.sub(r'\\b\\d+\\b', '', text)\n",
        "    # Step 6: Remove punctuation/special characters\n",
        "    text    = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Step 7: Tokenize (use spaCy)\n",
        "    doc = nlp(text)\n",
        "    # Step 8 & 9: Remove stop words and lemmatize\n",
        "    tokens  = [token.lemma_.lower() for token in doc\n",
        "               if not token.is_stop\n",
        "               and not token.is_punct\n",
        "               and not token.is_space]\n",
        "    return tokens\n",
        "\n",
        "    # Step 10: Remove short tokens\n",
        "    tokens = [token for token in tokens if len(token) >= 2]\n",
        "    pass  # Remove this line and return your tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99E-dCkDWPGx",
        "outputId": "e45d6cfb-df35-4084-c41b-79c24482565e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed tokens:\n",
            "['break', 'news', 'researcher', 'publish', 'new', 'paper', 'natural', 'language', 'processing', 'check', 'detail', 'contact', 'collaboration', 'experiment', 'conduct', 'stateoftheart', 'transformer', 'model', 'achieve', 'accuracy', 'benchmark', 'dataset']\n",
            "\n",
            "âœ… All tests passed!\n"
          ]
        }
      ],
      "source": [
        "# Test your pipeline with this text\n",
        "test_text = \"\"\"\n",
        "ðŸš€ BREAKING NEWS!!! The researchers at @MIT have published 5 new papers\n",
        "about Natural Language Processing! Check out https://mit.edu/nlp for details.\n",
        "Contact them at research@mit.edu for collaborations. #NLP #AI #Research\n",
        "\n",
        "The experiments were conducted using state-of-the-art transformers models.\n",
        "They achieved 95.5% accuracy on the benchmark datasets!!!\n",
        "\"\"\"\n",
        "\n",
        "result = preprocess_text(test_text)\n",
        "print(\"Preprocessed tokens:\")\n",
        "print(result)\n",
        "\n",
        "# Verify some expected tokens are in the result\n",
        "expected_tokens = ['researcher', 'publish', 'paper', 'natural', 'language', 'processing']\n",
        "for token in expected_tokens:\n",
        "    assert token in result, f\"Expected '{token}' in result\"\n",
        "\n",
        "# Verify unwanted elements are NOT in result\n",
        "unwanted = ['@mit', 'https', 'mit.edu', '#nlp', '95.5', '!!!', 'the', 'a', 'at']\n",
        "for item in unwanted:\n",
        "    assert item not in result, f\"'{item}' should not be in result\"\n",
        "\n",
        "print(\"\\nâœ… All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk3eF30TWPGx"
      },
      "source": [
        "## 4.3 Applying Pipeline to Multiple Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4-aYi1FWPGx",
        "outputId": "2679bf21-7772-474f-9c08-dda16b63632f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed documents:\n",
            "  Doc 1: ['machine', 'learning', 'transform', 'tech', 'industry']\n",
            "  Doc 2: ['love', 'program', 'python', 'easy', 'learn']\n",
            "  Doc 3: ['cat', 'sleep', 'couch', 'lazy']\n",
            "  Doc 4: ['contact', 'question', 'ai', 'product']\n"
          ]
        }
      ],
      "source": [
        "# TODO: Exercise 4.2\n",
        "# Apply your preprocessing pipeline to a list of documents\n",
        "# Return a list of lists (one list of tokens per document)\n",
        "\n",
        "documents = [\n",
        "    \"Machine learning is transforming the tech industry! @Google #ML\",\n",
        "    \"I love programming in Python. It's so easy to learn! https://python.org\",\n",
        "    \"The cats are sleeping on the couch. They're so lazy!\",\n",
        "    \"Contact support@company.com for any questions about our AI products.\"\n",
        "]\n",
        "\n",
        "# Apply your pipeline to each document\n",
        "processed_docs = [preprocess_text(doc) for doc in documents]\n",
        "\n",
        "print(\"Processed documents:\")\n",
        "for i, doc in enumerate(processed_docs):\n",
        "    print(f\"  Doc {i+1}: {doc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anvmcByZWPGx"
      },
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "### Tokenization\n",
        "- **Basic**: `str.split()` - simple but limited\n",
        "- **NLTK**: `word_tokenize()`, `sent_tokenize()` - handles punctuation\n",
        "- **spaCy**: `nlp(text)` - provides rich token information\n",
        "- **Custom**: `RegexpTokenizer` - for specific patterns\n",
        "\n",
        "### Normalization\n",
        "- **Stemming**: Fast, rule-based (Porter, Snowball) - may produce non-words\n",
        "- **Lemmatization**: Accurate, vocabulary-based - produces valid words\n",
        "- **spaCy**: Automatic POS-aware lemmatization with `token.lemma_`\n",
        "\n",
        "### Filtering\n",
        "- **Stop words**: NLTK `stopwords.words()`, spaCy `token.is_stop`\n",
        "- **Punctuation**: `string.punctuation`, spaCy `token.is_punct`\n",
        "- **Special characters**: regex `re.sub()`\n",
        "\n",
        "### Pipeline Best Practices\n",
        "1. Order matters! (lowercase before regex, tokenize before lemmatize)\n",
        "2. Choose stemming vs lemmatization based on your task\n",
        "3. Consider what stop words to keep (e.g., \"not\" for sentiment)\n",
        "4. Test your pipeline on sample data\n",
        "\n",
        "---\n",
        "\n",
        "## Submission\n",
        "\n",
        "1. Make sure all exercises are completed\n",
        "2. Save this notebook\n",
        "3. Create a Git repository and push your work\n",
        "4. **Send the repository link to: yoroba93@gmail.com**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp-course",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}